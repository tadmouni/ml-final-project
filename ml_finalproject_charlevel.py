# -*- coding: utf-8 -*-
"""ML_finalProject_CharLevel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SskruLoTkEtk8hPmjjefX5qAQD52ZIMe
"""

#Load Dataset
def loadDataset():
  df = pd.read_csv('vanityPlates.csv')

  return df

# Split DataFrame into Data and Target
def getDataTargetSets(df):
  temp = df
  status_list = df['status']
  target_df = pd.DataFrame(status_list, columns = ['status']) 
  temp.drop(['status'], axis=1 , inplace=True)

  return temp, target_df

# Check profanity probability of each vanity plate
def runProfanityCheck(df):
  plateArr = df['plate']
  plate_prof_probs = []
  for plate in plateArr:
    plate_prof_probs.append(predict_prob([plate])[0])
  return plate_prof_probs

# Perform one hot encoding for vanity plate text
def one_hot_encode_plate(df):
  alphaNumChars="ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789@#$%^&* "
  plate_texts = df['plate'].values
  for char in alphaNumChars:
    newPlateColName = "plate_"+char
    plateChars = []
    for plate in plate_texts:
      if char in plate:
        plateChars.append('1')
      else:
        plateChars.append('0')
    df[newPlateColName] = plateChars

  return df

# Perform one hot encoding for review reason code
def one_hot_encode_review_reason_code(df):

  df = pd.concat([df, pd.get_dummies(df['review_reason_code'], prefix='review_reason_code')], axis=1)
  df.drop(['review_reason_code'], axis=1, inplace=True)
  
  return df

def train_test_split(X, y):
  from sklearn.model_selection import train_test_split

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42, stratify=y)
  print("No. of samples in X_train: ", len(X_train))
  print("No. of samples in X_test: ", len(X_test))
  print("No. of samples in y_train: ", len(y_train))
  print("No. of samples in y_test: ", len(y_test))

  return X_train, X_test, y_train, y_test

#!pip install profanity-check
import pandas as pd
import numpy as np
#from profanity_check import predict, predict_prob

# Load the dataset
df = loadDataset()

# One-Hot Encode Vanity Plates - Character level
df = one_hot_encode_plate(df)

# One-Hot Encode Review Reason Code
df = one_hot_encode_review_reason_code(df)

#cleanup: drop string columns so that PCA will work 
df.drop(['plate', 'customer_meaning', 'reviewer_comments'], axis=1, inplace=True)
print(df.shape)

# Making new data frame with dropped NA values
new_df = df.dropna(axis = 0, how ='any')
  
# comparing sizes of data frames 
print("Old data frame length:", len(df), "\nNew data frame length:",  
       len(new_df), "\nNumber of rows with at least 1 NA value: ", 
       (len(df)-len(new_df))) 

#Split dataset into data and target
data_df, target_df = getDataTargetSets(new_df)

data_df.shape

# Status count in dataset
print(target_df['status'].value_counts())

# Split Dataset into train and test
X = data_df.to_numpy()
y = target_df.to_numpy()

print(X)
print(y)
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Commented out IPython magic to ensure Python compatibility.
#determine number of features with PCA evaluating on simple DT model with stratified cross validation

from sklearn.decomposition import PCA
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
# %matplotlib inline

numfeatures = []
recall_pca = []
best_recall_pca = 0

for features in [120, 100, 80, 60, 40, 35, 30, 25, 20, 15, 5, 1]:
  #dimensionality reduction
  pca = PCA(n_components=features, random_state = 42)
  pca.fit(X_train)
  X_train_reduced = pca.transform(X_train)

  #Decision tree
  DT_pca = DecisionTreeClassifier()
  DT_pca_recalls = cross_val_score(DT_pca, X_train_reduced, y_train, cv=5, scoring='recall_macro')
  DT_pca_recalls_avg = DT_pca_recalls.mean()
  
  if DT_pca_recalls_avg > best_recall_pca:
    best_recall_pca = DT_pca_recalls_avg
    best_param_pca = {'n_components': features}

  numfeatures.append(features)
  recall_pca.append(DT_pca_recalls_avg)

print(best_recall_pca)
print(best_param_pca)
plt.plot(numfeatures, recall_pca)
plt.xlabel("number of features")
plt.ylabel("recall")


#dimensionality reduction
pca = PCA(**best_param_pca)
pca.fit(X_train)
X_train_reduced = pca.transform(X_train)
X_test_reduced = pca.transform(X_test)

# Perform 5-Fold Stratified cross-validation

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

skFold = StratifiedKFold(n_splits = 5)

# Perform 5-Fold Stratified cross-validation

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

skFold = StratifiedKFold(n_splits = 5)

#Decision Tree classifier
from sklearn.tree import DecisionTreeClassifier

recall_dt = []
best_score_dt = 0
criterion_settings = ["gini", "entropy"]
depth_settings = range(5, 30)

for criterion_value in criterion_settings:
  for depth_value in depth_settings:
    # Build model
    dTreeClassifier = DecisionTreeClassifier(criterion=criterion_value, max_depth=depth_value, random_state = 42)

    fold_recall_dt = cross_val_score(dTreeClassifier, X_train_reduced, y_train, scoring='recall_macro', cv=skFold)
    #print("Cross_validation_score: \n{}".format(fold_recall_dt.mean()))
    score = fold_recall_dt.mean()
    recall_dt.append(score)
    if score > best_score_dt:
      best_param_dt = {'criterion': criterion_value, 'max_depth': depth_value}
      best_score_dt = score

print(best_param_dt)
print(best_score_dt)

# Tune hyperparameters for SVM using stratified 5-fold cross-validation
from sklearn.svm import SVC

best_score_svm = 0
degree_settings = range(2,4)
#gamma_settings = ["auto", "scale"]
gamma_settings = ["auto"]
#c_settings = [1e-9, 1e-6, 0.00001, 0.0001, 0.001]
c_settings = [1e-12, 1e-9, 1e-6, 0.00001]

for degree_value in degree_settings:
  for gamma_value in gamma_settings:
    for c_value in c_settings:
    # Build model
      svmClassifier = SVC(kernel="poly", degree=degree_value, gamma=gamma_value, C=c_value, random_state=42)

      fold_recall_svm = cross_val_score(svmClassifier, X_train_reduced, np.ravel(y_train,order='C'), scoring='recall_macro', cv=skFold)
      #print("Cross_validation_score: \n{}".format(fold_recall_svm.mean()))
      score = fold_recall_svm.mean()
      if score > best_score_svm:
        best_param_svm = {'degree': degree_value, 'gamma': gamma_value, 'C': c_value, 'kernel':'poly'}
        best_score_svm = score

print(best_score_svm)
print(best_param_svm)

# Train Naive Bayes classifier
from sklearn.naive_bayes import GaussianNB

gaussian_model = GaussianNB()
fold_recall_nb = cross_val_score(gaussian_model, X_train_reduced, np.ravel(y_train,order='C'), scoring='recall_macro', cv=skFold)
score_nb = fold_recall_nb.mean()
print(score_nb)

#Retrain DecisionTree
dTreeClassifier = DecisionTreeClassifier(**best_param_dt, random_state = 42)
dTreeClassifier.fit(X_train_reduced, np.ravel(y_train,order='C'))

#Retrain SVM
svmClassifier = SVC(**best_param_svm, random_state = 42)
svmClassifier.fit(X_train_reduced, np.ravel(y_train,order='C'))

# Experiment 3 - Ensemble Learning
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier

# Majority Vote Classifier
enClf = VotingClassifier(estimators=[('dt', dTreeClassifier),('gnb', gaussian_model)], voting='hard')
fold_precision_mvote = cross_val_score(enClf, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='precision_macro')
fold_recall_mvote = cross_val_score(enClf, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
print("Majority Vote Classifier")
print(fold_precision_mvote)
print(fold_recall_mvote)
print(fold_precision_mvote.mean())
print(fold_recall_mvote.mean())

# Bagging classifier for DecisionTree
baggingClf_dt = BaggingClassifier(dTreeClassifier, random_state = 42)

fold_precision_bag_dt = cross_val_score(baggingClf_dt, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='precision_macro')
fold_recall_bag_dt = cross_val_score(baggingClf_dt, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
print("Bagging Classifier - Decision Tree")
print(fold_precision_bag_dt)
print(fold_recall_bag_dt)
print(fold_precision_bag_dt.mean())
print(fold_recall_bag_dt.mean())

# Bagging classifier for SVM
baggingClf_svm = BaggingClassifier(svmClassifier, random_state = 42)
fold_precision_bag_svm = cross_val_score(baggingClf_svm, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='precision_macro')
fold_recall_bag_svm = cross_val_score(baggingClf_svm, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
print("Bagging Classifier - SVM")
print(fold_precision_bag_svm)
print(fold_recall_bag_svm)
print(fold_precision_bag_svm.mean())
print(fold_recall_bag_svm.mean())

# Bagging classifier for NaiveBayesGaussian
baggingClf_gnb = BaggingClassifier(gaussian_model, random_state = 42)
fold_precision_bag_gnb = cross_val_score(baggingClf_gnb, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='precision_macro')
fold_recall_bag_gnb = cross_val_score(baggingClf_gnb, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
print("Bagging Classifier - NaiveBayes")
print(fold_precision_bag_gnb)
print(fold_recall_bag_gnb)
print(fold_precision_bag_gnb.mean())
print(fold_recall_bag_gnb.mean())


# AdaBoost classifier for DecisionTree
adabooster_dt = AdaBoostClassifier(base_estimator=dTreeClassifier, n_estimators = 50, random_state = 42)
fold_precision_boost_dt = cross_val_score(adabooster_dt, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='precision_macro')
fold_recall_boost_dt = cross_val_score(adabooster_dt, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
print("Boosting Classifier - DecisionTree")
print(fold_precision_boost_dt)
print(fold_recall_boost_dt)
print(fold_precision_boost_dt.mean())
print(fold_recall_boost_dt.mean())

# AdaBoost classifier for NaiveBayes
adabooster_gnb = AdaBoostClassifier(base_estimator=gaussian_model, n_estimators = 50, random_state = 42)
fold_precision_boost_gnb = cross_val_score(adabooster_gnb, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='precision_macro')
fold_recall_boost_gnb = cross_val_score(adabooster_gnb, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
print("Boosting Classifier - NaiveBayes")
print(fold_precision_boost_gnb)
print(fold_recall_boost_gnb)
print(fold_precision_boost_gnb.mean())
print(fold_recall_boost_gnb.mean())

# Commented out IPython magic to ensure Python compatibility.
# experiment 4: train/determine hyperparameters for MLPClassifier 
from sklearn.neural_network import MLPClassifier

#optimize hidden layers and # of neurons
recall_1hl = []
recall_2hl = []
recall_3hl = []
n_neurons_per_l = []
for n_hidden_units in [10, 50 , 100, 150, 200]:
  n_neurons_per_l.append(n_hidden_units)
  #one hidden layer
  mlp1 = MLPClassifier(activation='relu', max_iter=200, hidden_layer_sizes= [n_hidden_units], verbose=True, random_state=42)
  mlp1_recalls = cross_val_score(mlp1, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
  mlp1_recalls_avg = mlp1_recalls.mean()
  recall_1hl.append(mlp1_recalls_avg)
  #two hidden layers
  mlp2 = MLPClassifier(activation='relu', hidden_layer_sizes= [n_hidden_units, n_hidden_units], max_iter=200, verbose=True, random_state=42)
  mlp2_recalls = cross_val_score(mlp2, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
  mlp2_recalls_avg = mlp2_recalls.mean()
  recall_2hl.append(mlp2_recalls_avg)
  #three hidden layers
  mlp3 = MLPClassifier(activation='relu', hidden_layer_sizes= [n_hidden_units, n_hidden_units, n_hidden_units], max_iter=200, verbose=True, random_state=42)
  mlp3_recalls = cross_val_score(mlp3, X_train_reduced, np.ravel(y_train,order='C'), cv=skFold, scoring='recall_macro')
  mlp3_recalls_avg = mlp3_recalls.mean()
  recall_3hl.append(mlp3_recalls_avg)

#plot
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(n_neurons_per_l, recall_1hl, label="1 hidden layer")
plt.plot(n_neurons_per_l, recall_2hl, label="2 hidden layers")
plt.plot(n_neurons_per_l, recall_3hl, label="3 hidden layers")
plt.xlabel("Neurons per layer")
plt.ylabel("Recall")
plt.legend()

# Commented out IPython magic to ensure Python compatibility.
parameters={
'hidden_layer_sizes': [x for x in itertools.product((50, 100, 250, 400),repeat=3)]
}

clf
= gridSearchCV(estimator=MLPClassifier,param_grid=parameters,n_jobs=-1,verbose=2,cv=10)
# experiment 4: train/determine hyperparameters for MLPClassifier 
from sklearn.neural_network import MLPClassifier

#optimize hidden layers and # of neurons
recall_1hl = []
recall_2hl = []
recall_3hl = []
n_neurons_per_l = []
for n_hidden_units in [10, 25, 50, 100]:
  n_neurons_per_l.append(n_hidden_units)
  #one hidden layer
  mlp1 = MLPClassifier(hidden_layer_sizes= [n_hidden_units], max_iter=50)
  mlp1_recalls = cross_val_score(mlp1, X_train, y_train, cv=5, scoring='recall_micro')
  mlp1_recalls_avg = mlp1_recalls.mean()
  recall_1hl.append(mlp1_recalls_avg)
  #two hidden layers
  mlp2 = MLPClassifier(hidden_layer_sizes= [n_hidden_units, n_hidden_units], max_iter=50)
  mlp2_recalls = cross_val_score(mlp2, X_train, y_train, cv=5, scoring='recall_macro')
  mlp2_recalls_avg = mlp2_recalls.mean()
  recall_2hl.append(mlp2_recalls_avg)
  #three hidden layers
  mlp3 = MLPClassifier(hidden_layer_sizes= [n_hidden_units, n_hidden_units, n_hidden_units], max_iter=50)
  mlp3_recalls = cross_val_score(mlp3, X_train, y_train, cv=5, scoring='recall_macro')
  mlp3_recalls_avg = mlp3_recalls.mean()
  recall_3hl.append(mlp3_recalls_avg)


#plot
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(n_neurons_per_l, recall_1hl, label="1 hidden layer")
plt.plot(n_neurons_per_l, recall_2hl, label="2 hidden layers")
plt.plot(n_neurons_per_l, recall_3hl, label="3 hidden layers")
plt.xlabel("Neurons per layer")
plt.ylabel("Recall")
plt.legend()

from sklearn.neural_network import MLPClassifier

from sklearn.model_selection import GridSearchCV
parameter_space = {
    'max_iter':[400],
    'hidden_layer_sizes': [(200),(300),(400)],
    'alpha': [0.0001, 1e-5, 1e-6]
}

#MLP without PCA
clf = GridSearchCV(MLPClassifier(), parameter_space, n_jobs=-1, cv=skFold, scoring='recall_macro')
clf.fit(X_train, y_train)
print(clf.cv_results_)
print(clf.score(X_train, y_train))
print(clf.best_params_)
print(clf.best_score_)

# MLP with PCA
clf.fit(X_train_reduced, y_train)
print(clf.cv_results_)
print(clf.score(X_train_reduced, y_train))
print(clf.best_params_)
print(clf.best_score_)

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns;

dTreeClassifier.fit(X_train_reduced, np.ravel(y_train,order='C'))
print(dTreeClassifier)
y_pred_dt = dTreeClassifier.predict(X_test_reduced)
print(classification_report(y_test, y_pred_dt))

# DecisionTree confusion matrix
mat_dt = confusion_matrix(y_test, y_pred_dt)
sns.heatmap(mat_dt.T, square=True, annot=True, fmt='d', cbar=False)
plt.title('DecisionTree Confusion Matrix')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.show()

#SVM

svmClassifier.fit(X_train_reduced, np.ravel(y_train,order='C'))
print(svmClassifier)
y_pred_svm = svmClassifier.predict(X_test_reduced)
print(classification_report(y_test, y_pred_svm))

# SVM confusion matrix
mat_svm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(mat_svm.T, square=True, annot=True, fmt='d', cbar=False)
plt.title('SVM Confusion Matrix')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.show()

baggingClf_dt.fit(X_train_reduced, np.ravel(y_train,order='C'))
print(baggingClf_dt)
y_pred_bag_dt = baggingClf_dt.predict(X_test_reduced)
print(classification_report(y_test, y_pred_bag_dt))

# Bagging Clf Decision Tree confusion matrix
mat_bag_dt = confusion_matrix(y_test, y_pred_bag_dt)
sns.heatmap(mat_bag_dt.T, square=True, annot=True, fmt='d', cbar=False)
plt.title('Bagging Classifier DecisionTree Confusion Matrix')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.show()

adabooster_dt.fit(X_train_reduced, np.ravel(y_train,order='C'))
print(adabooster_dt)
y_pred_boost_dt = adabooster_dt.predict(X_test_reduced)
print(classification_report(y_test, y_pred_boost_dt))

# Bagging Clf Decision Tree confusion matrix
mat_bag_dt = confusion_matrix(y_test, y_pred_boost_dt)
sns.heatmap(mat_bag_dt.T, square=True, annot=True, fmt='d', cbar=False)
plt.title('Boosting Classifier DecisionTree Confusion Matrix')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.show()

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns;
import matplotlib.pyplot as plt

mlp = MLPClassifier(hidden_layer_sizes= [200], max_iter=400, alpha=0.0001, random_state = 42)
mlp.fit(X_train, y_train)

print(mlp)
y_pred_mlp = mlp.predict(X_test)
print(classification_report(y_test, y_pred_mlp))

# MLP confusion matrix
mat_mlp = confusion_matrix(y_test, y_pred_mlp)
sns.heatmap(mat_mlp.T, square=True, annot=True, fmt='d', cbar=False)
plt.title('MLP Confusion Matrix')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.show()

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns;

mlp_1 = MLPClassifier(batch_size='auto', hidden_layer_sizes= [300], max_iter=400, alpha=0.0001, random_state = 42)
mlp_1.fit(X_train, np.ravel(y_train,order='C'))

print(mlp_1)
y_pred_mlp = mlp_1.predict(X_test)
print(classification_report(y_test, y_pred_mlp))

# MLP confusion matrix
mat_mlp = confusion_matrix(y_test, y_pred_mlp)
sns.heatmap(mat_mlp.T, square=True, annot=True, fmt='d', cbar=False)
plt.title('MLP Confusion Matrix')
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.show()